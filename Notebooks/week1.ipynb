{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Activity 1: Simple 2-gram Language Model\n",
    "\n",
    "In this activity, we'll implement a simple 2-gram language model with both word-level and character-level tokenization. We'll:\n",
    "1. Implement tokenization\n",
    "2. Collect counts\n",
    "3. Calculate probabilities for new sentences\n",
    "4. Add interpolation with unigram probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization Functions\n",
    "\n",
    "Let's implement both word-level and character-level tokenization with special tokens for sentence boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word tokens: ['<BOS>', 'hello', 'world!', '<EOS>']\n",
      "Char tokens: ['<BOS>', 'h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "def word_tokenize(text):\n",
    "    # Word tokenization with BOS and EOS tokens\n",
    "    tokens = text.lower().split()\n",
    "    return ['<BOS>'] + tokens + ['<EOS>']\n",
    "\n",
    "def char_tokenize(text):\n",
    "    # Character tokenization with BOS and EOS tokens\n",
    "    chars = list(text.lower())\n",
    "    return ['<BOS>'] + chars + ['<EOS>']\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello world!\"\n",
    "print(\"Word tokens:\", word_tokenize(text))\n",
    "print(\"Char tokens:\", char_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram Model with Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramModel:\n",
    "    def __init__(self, tokenize_fn, lambda_1=0.8, lambda_2=0.2):\n",
    "        self.tokenize = tokenize_fn\n",
    "        self.unigram_counts = defaultdict(int)\n",
    "        self.bigram_counts = defaultdict(int)\n",
    "        self.total_tokens = 0\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.lambda_2 = lambda_2\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "    def train(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "            \n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            \n",
    "            # Build vocabulary and count unigrams\n",
    "            for token in tokens:\n",
    "                self.vocabulary.add(token)\n",
    "                self.unigram_counts[token] += 1\n",
    "                self.total_tokens += 1\n",
    "            \n",
    "            # Count bigrams\n",
    "            for i in range(len(tokens)-1):\n",
    "                bigram = (tokens[i], tokens[i+1])\n",
    "                self.bigram_counts[bigram] += 1\n",
    "    \n",
    "    def get_unigram_prob(self, token):\n",
    "        # Handle unseen tokens with a small probability\n",
    "        if token not in self.vocabulary:\n",
    "            return 1.0 / (self.total_tokens + len(self.vocabulary))\n",
    "        return self.unigram_counts[token] / self.total_tokens\n",
    "    \n",
    "    def get_bigram_prob(self, token1, token2):\n",
    "        bigram = (token1, token2)\n",
    "        if token1 not in self.vocabulary:\n",
    "            return 0.0\n",
    "        denominator = self.unigram_counts[token1]\n",
    "        if denominator == 0:\n",
    "            return 0.0\n",
    "        return self.bigram_counts[bigram] / denominator\n",
    "    \n",
    "    def get_interpolated_prob(self, token1, token2):\n",
    "        bigram_prob = self.get_bigram_prob(token1, token2)\n",
    "        unigram_prob = self.get_unigram_prob(token2)\n",
    "        return self.lambda_1 * bigram_prob + self.lambda_2 * unigram_prob\n",
    "    \n",
    "    def get_sequence_prob(self, text, use_interpolation=True):\n",
    "        tokens = self.tokenize(text)\n",
    "        if len(tokens) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        prob = 1.0\n",
    "        for i in range(len(tokens)-1):\n",
    "            if use_interpolation:\n",
    "                prob *= self.get_interpolated_prob(tokens[i], tokens[i+1])\n",
    "            else:\n",
    "                prob *= self.get_bigram_prob(tokens[i], tokens[i+1])\n",
    "        return prob\n",
    "    \n",
    "    def analyze_sequence(self, text, use_interpolation=True):\n",
    "        tokens = self.tokenize(text)\n",
    "        print(f\"\\nAnalyzing sequence: {text}\")\n",
    "        print(\"Token sequence:\", tokens)\n",
    "        print(\"\\nProbability breakdown:\")\n",
    "        \n",
    "        total_prob = 1.0\n",
    "        for i in range(len(tokens)-1):\n",
    "            token1, token2 = tokens[i], tokens[i+1]\n",
    "            \n",
    "            if use_interpolation:\n",
    "                bigram_prob = self.get_bigram_prob(token1, token2)\n",
    "                unigram_prob = self.get_unigram_prob(token2)\n",
    "                interpolated_prob = self.get_interpolated_prob(token1, token2)\n",
    "                \n",
    "                print(f\"\\nTransition {token1} → {token2}:\")\n",
    "                print(f\"  Bigram P({token2}|{token1}) = {bigram_prob:.4f}\")\n",
    "                print(f\"  Unigram P({token2}) = {unigram_prob:.4f}\")\n",
    "                print(f\"  Interpolated = {self.lambda_1:.2f} * {bigram_prob:.4f} + {self.lambda_2:.2f} * {unigram_prob:.4f} = {interpolated_prob:.4f}\")\n",
    "                \n",
    "                total_prob *= interpolated_prob\n",
    "            else:\n",
    "                prob = self.get_bigram_prob(token1, token2)\n",
    "                print(f\"P({token2}|{token1}) = {prob:.4f}\")\n",
    "                total_prob *= prob\n",
    "                \n",
    "        print(f\"\\nTotal sequence probability: {total_prob:.6f}\")\n",
    "        return total_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Testing\n",
    "\n",
    "Let's train both word-level and character-level models on some example text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "training_texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ran in the park\",\n",
    "    \"a cat and a dog played\"\n",
    "]\n",
    "\n",
    "# Create and train word-level model\n",
    "word_model = BigramModel(word_tokenize)\n",
    "word_model.train(training_texts)\n",
    "\n",
    "# Create and train character-level model\n",
    "char_model = BigramModel(char_tokenize, lambda_1=0.7, lambda_2=0.3)\n",
    "char_model.train(training_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Different Sequences\n",
    "\n",
    "Let's test both models on some sentences and compare their probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing interpolated probabilities for different sequences:\n",
      "\n",
      "==================================================\n",
      "Sequence: the cat sat\n",
      "\n",
      "Word-level analysis:\n",
      "\n",
      "Analyzing sequence: the cat sat\n",
      "Token sequence: ['<BOS>', 'the', 'cat', 'sat', '<EOS>']\n",
      "\n",
      "Probability breakdown:\n",
      "\n",
      "Transition <BOS> → the:\n",
      "  Bigram P(the|<BOS>) = 0.6667\n",
      "  Unigram P(the) = 0.1667\n",
      "  Interpolated = 0.80 * 0.6667 + 0.20 * 0.1667 = 0.5667\n",
      "\n",
      "Transition the → cat:\n",
      "  Bigram P(cat|the) = 0.2500\n",
      "  Unigram P(cat) = 0.0833\n",
      "  Interpolated = 0.80 * 0.2500 + 0.20 * 0.0833 = 0.2167\n",
      "\n",
      "Transition cat → sat:\n",
      "  Bigram P(sat|cat) = 0.5000\n",
      "  Unigram P(sat) = 0.0417\n",
      "  Interpolated = 0.80 * 0.5000 + 0.20 * 0.0417 = 0.4083\n",
      "\n",
      "Transition sat → <EOS>:\n",
      "  Bigram P(<EOS>|sat) = 0.0000\n",
      "  Unigram P(<EOS>) = 0.1250\n",
      "  Interpolated = 0.80 * 0.0000 + 0.20 * 0.1250 = 0.0250\n",
      "\n",
      "Total sequence probability: 0.001253\n",
      "\n",
      "Character-level analysis:\n",
      "\n",
      "Analyzing sequence: the cat sat\n",
      "Token sequence: ['<BOS>', 't', 'h', 'e', ' ', 'c', 'a', 't', ' ', 's', 'a', 't', '<EOS>']\n",
      "\n",
      "Probability breakdown:\n",
      "\n",
      "Transition <BOS> → t:\n",
      "  Bigram P(t|<BOS>) = 0.6667\n",
      "  Unigram P(t) = 0.1096\n",
      "  Interpolated = 0.70 * 0.6667 + 0.30 * 0.1096 = 0.4995\n",
      "\n",
      "Transition t → h:\n",
      "  Bigram P(h|t) = 0.5000\n",
      "  Unigram P(h) = 0.0548\n",
      "  Interpolated = 0.70 * 0.5000 + 0.30 * 0.0548 = 0.3664\n",
      "\n",
      "Transition h → e:\n",
      "  Bigram P(e|h) = 1.0000\n",
      "  Unigram P(e) = 0.0685\n",
      "  Interpolated = 0.70 * 1.0000 + 0.30 * 0.0685 = 0.7205\n",
      "\n",
      "Transition e →  :\n",
      "  Bigram P( |e) = 0.8000\n",
      "  Unigram P( ) = 0.2055\n",
      "  Interpolated = 0.70 * 0.8000 + 0.30 * 0.2055 = 0.6216\n",
      "\n",
      "Transition   → c:\n",
      "  Bigram P(c| ) = 0.1333\n",
      "  Unigram P(c) = 0.0274\n",
      "  Interpolated = 0.70 * 0.1333 + 0.30 * 0.0274 = 0.1016\n",
      "\n",
      "Transition c → a:\n",
      "  Bigram P(a|c) = 1.0000\n",
      "  Unigram P(a) = 0.1370\n",
      "  Interpolated = 0.70 * 1.0000 + 0.30 * 0.1370 = 0.7411\n",
      "\n",
      "Transition a → t:\n",
      "  Bigram P(t|a) = 0.4000\n",
      "  Unigram P(t) = 0.1096\n",
      "  Interpolated = 0.70 * 0.4000 + 0.30 * 0.1096 = 0.3129\n",
      "\n",
      "Transition t →  :\n",
      "  Bigram P( |t) = 0.3750\n",
      "  Unigram P( ) = 0.2055\n",
      "  Interpolated = 0.70 * 0.3750 + 0.30 * 0.2055 = 0.3241\n",
      "\n",
      "Transition   → s:\n",
      "  Bigram P(s| ) = 0.0667\n",
      "  Unigram P(s) = 0.0137\n",
      "  Interpolated = 0.70 * 0.0667 + 0.30 * 0.0137 = 0.0508\n",
      "\n",
      "Transition s → a:\n",
      "  Bigram P(a|s) = 1.0000\n",
      "  Unigram P(a) = 0.1370\n",
      "  Interpolated = 0.70 * 1.0000 + 0.30 * 0.1370 = 0.7411\n",
      "\n",
      "Transition a → t:\n",
      "  Bigram P(t|a) = 0.4000\n",
      "  Unigram P(t) = 0.1096\n",
      "  Interpolated = 0.70 * 0.4000 + 0.30 * 0.1096 = 0.3129\n",
      "\n",
      "Transition t → <EOS>:\n",
      "  Bigram P(<EOS>|t) = 0.1250\n",
      "  Unigram P(<EOS>) = 0.0411\n",
      "  Interpolated = 0.70 * 0.1250 + 0.30 * 0.0411 = 0.0998\n",
      "\n",
      "Total sequence probability: 0.000001\n",
      "\n",
      "==================================================\n",
      "Sequence: xyz\n",
      "\n",
      "Word-level analysis:\n",
      "\n",
      "Analyzing sequence: xyz\n",
      "Token sequence: ['<BOS>', 'xyz', '<EOS>']\n",
      "\n",
      "Probability breakdown:\n",
      "\n",
      "Transition <BOS> → xyz:\n",
      "  Bigram P(xyz|<BOS>) = 0.0000\n",
      "  Unigram P(xyz) = 0.0263\n",
      "  Interpolated = 0.80 * 0.0000 + 0.20 * 0.0263 = 0.0053\n",
      "\n",
      "Transition xyz → <EOS>:\n",
      "  Bigram P(<EOS>|xyz) = 0.0000\n",
      "  Unigram P(<EOS>) = 0.1250\n",
      "  Interpolated = 0.80 * 0.0000 + 0.20 * 0.1250 = 0.0250\n",
      "\n",
      "Total sequence probability: 0.000132\n",
      "\n",
      "Character-level analysis:\n",
      "\n",
      "Analyzing sequence: xyz\n",
      "Token sequence: ['<BOS>', 'x', 'y', 'z', '<EOS>']\n",
      "\n",
      "Probability breakdown:\n",
      "\n",
      "Transition <BOS> → x:\n",
      "  Bigram P(x|<BOS>) = 0.0000\n",
      "  Unigram P(x) = 0.0108\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0108 = 0.0032\n",
      "\n",
      "Transition x → y:\n",
      "  Bigram P(y|x) = 0.0000\n",
      "  Unigram P(y) = 0.0137\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0137 = 0.0041\n",
      "\n",
      "Transition y → z:\n",
      "  Bigram P(z|y) = 0.0000\n",
      "  Unigram P(z) = 0.0108\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0108 = 0.0032\n",
      "\n",
      "Transition z → <EOS>:\n",
      "  Bigram P(<EOS>|z) = 0.0000\n",
      "  Unigram P(<EOS>) = 0.0411\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0411 = 0.0123\n",
      "\n",
      "Total sequence probability: 0.000000\n",
      "\n",
      "==================================================\n",
      "Sequence: the xyz cat\n",
      "\n",
      "Word-level analysis:\n",
      "\n",
      "Analyzing sequence: the xyz cat\n",
      "Token sequence: ['<BOS>', 'the', 'xyz', 'cat', '<EOS>']\n",
      "\n",
      "Probability breakdown:\n",
      "\n",
      "Transition <BOS> → the:\n",
      "  Bigram P(the|<BOS>) = 0.6667\n",
      "  Unigram P(the) = 0.1667\n",
      "  Interpolated = 0.80 * 0.6667 + 0.20 * 0.1667 = 0.5667\n",
      "\n",
      "Transition the → xyz:\n",
      "  Bigram P(xyz|the) = 0.0000\n",
      "  Unigram P(xyz) = 0.0263\n",
      "  Interpolated = 0.80 * 0.0000 + 0.20 * 0.0263 = 0.0053\n",
      "\n",
      "Transition xyz → cat:\n",
      "  Bigram P(cat|xyz) = 0.0000\n",
      "  Unigram P(cat) = 0.0833\n",
      "  Interpolated = 0.80 * 0.0000 + 0.20 * 0.0833 = 0.0167\n",
      "\n",
      "Transition cat → <EOS>:\n",
      "  Bigram P(<EOS>|cat) = 0.0000\n",
      "  Unigram P(<EOS>) = 0.1250\n",
      "  Interpolated = 0.80 * 0.0000 + 0.20 * 0.1250 = 0.0250\n",
      "\n",
      "Total sequence probability: 0.000001\n",
      "\n",
      "Character-level analysis:\n",
      "\n",
      "Analyzing sequence: the xyz cat\n",
      "Token sequence: ['<BOS>', 't', 'h', 'e', ' ', 'x', 'y', 'z', ' ', 'c', 'a', 't', '<EOS>']\n",
      "\n",
      "Probability breakdown:\n",
      "\n",
      "Transition <BOS> → t:\n",
      "  Bigram P(t|<BOS>) = 0.6667\n",
      "  Unigram P(t) = 0.1096\n",
      "  Interpolated = 0.70 * 0.6667 + 0.30 * 0.1096 = 0.4995\n",
      "\n",
      "Transition t → h:\n",
      "  Bigram P(h|t) = 0.5000\n",
      "  Unigram P(h) = 0.0548\n",
      "  Interpolated = 0.70 * 0.5000 + 0.30 * 0.0548 = 0.3664\n",
      "\n",
      "Transition h → e:\n",
      "  Bigram P(e|h) = 1.0000\n",
      "  Unigram P(e) = 0.0685\n",
      "  Interpolated = 0.70 * 1.0000 + 0.30 * 0.0685 = 0.7205\n",
      "\n",
      "Transition e →  :\n",
      "  Bigram P( |e) = 0.8000\n",
      "  Unigram P( ) = 0.2055\n",
      "  Interpolated = 0.70 * 0.8000 + 0.30 * 0.2055 = 0.6216\n",
      "\n",
      "Transition   → x:\n",
      "  Bigram P(x| ) = 0.0000\n",
      "  Unigram P(x) = 0.0108\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0108 = 0.0032\n",
      "\n",
      "Transition x → y:\n",
      "  Bigram P(y|x) = 0.0000\n",
      "  Unigram P(y) = 0.0137\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0137 = 0.0041\n",
      "\n",
      "Transition y → z:\n",
      "  Bigram P(z|y) = 0.0000\n",
      "  Unigram P(z) = 0.0108\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.0108 = 0.0032\n",
      "\n",
      "Transition z →  :\n",
      "  Bigram P( |z) = 0.0000\n",
      "  Unigram P( ) = 0.2055\n",
      "  Interpolated = 0.70 * 0.0000 + 0.30 * 0.2055 = 0.0616\n",
      "\n",
      "Transition   → c:\n",
      "  Bigram P(c| ) = 0.1333\n",
      "  Unigram P(c) = 0.0274\n",
      "  Interpolated = 0.70 * 0.1333 + 0.30 * 0.0274 = 0.1016\n",
      "\n",
      "Transition c → a:\n",
      "  Bigram P(a|c) = 1.0000\n",
      "  Unigram P(a) = 0.1370\n",
      "  Interpolated = 0.70 * 1.0000 + 0.30 * 0.1370 = 0.7411\n",
      "\n",
      "Transition a → t:\n",
      "  Bigram P(t|a) = 0.4000\n",
      "  Unigram P(t) = 0.1096\n",
      "  Interpolated = 0.70 * 0.4000 + 0.30 * 0.1096 = 0.3129\n",
      "\n",
      "Transition t → <EOS>:\n",
      "  Bigram P(<EOS>|t) = 0.1250\n",
      "  Unigram P(<EOS>) = 0.0411\n",
      "  Interpolated = 0.70 * 0.1250 + 0.30 * 0.0411 = 0.0998\n",
      "\n",
      "Total sequence probability: 0.000000\n"
     ]
    }
   ],
   "source": [
    "test_sequences = [\n",
    "    \"the cat sat\",    # Seen sequence\n",
    "    \"xyz\",            # Completely unseen\n",
    "    \"the xyz cat\"     # Partially seen\n",
    "]\n",
    "\n",
    "print(\"Comparing interpolated probabilities for different sequences:\")\n",
    "for sequence in test_sequences:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Sequence: {sequence}\")\n",
    "    \n",
    "    print(\"\\nWord-level analysis:\")\n",
    "    word_model.analyze_sequence(sequence)\n",
    "    \n",
    "    print(\"\\nCharacter-level analysis:\")\n",
    "    char_model.analyze_sequence(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Points\n",
    "\n",
    "1. Compare the probabilities from word-level and character-level models:\n",
    "   - Why are they so different in scale?\n",
    "   - How does interpolation help with unseen sequences?\n",
    "\n",
    "2. Which model (word or character) is better at handling:\n",
    "   - Common phrases?\n",
    "   - Novel words?\n",
    "   - Partially seen sequences?\n",
    "\n",
    "3. What are the effects of different interpolation weights (λ₁ and λ₂)?\n",
    "   - Higher λ₁: More weight on context (bigrams)\n",
    "   - Higher λ₂: More weight on individual tokens (unigrams)\n",
    "\n",
    "4. How could we further improve this model?\n",
    "   - Using higher-order n-grams\n",
    "   - Different smoothing techniques\n",
    "   - Subword tokenization\n",
    "   - Neural language models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
