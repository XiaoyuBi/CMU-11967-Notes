{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Activity 1: Evaluating Machine Translation Outputs\n",
    "\n",
    "In this activity, we'll analyze machine translation outputs using different evaluation metrics (BLEU and chrF) and compare them with human evaluations. We'll:\n",
    "\n",
    "1. Load and examine WMT shared task data\n",
    "2. Calculate different automatic metrics\n",
    "3. Compare metric rankings with human evaluations\n",
    "4. Analyze cases where automatic and human evaluations differ significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load WMT Data\n",
    "\n",
    "We'll use the WMT metrics shared task data, which includes machine translations and human judgments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WMT metrics data\n",
    "print(\"\\nLoading WMT metrics dataset...\")\n",
    "dataset = load_dataset(\"nllg/wmt-metrics-data\")\n",
    "print(\"Successfully loaded WMT metrics dataset\")\n",
    "\n",
    "# Create a DataFrame with source, reference, and system outputs\n",
    "data = []\n",
    "max_samples = 100  # Limit samples for faster processing\n",
    "\n",
    "print(\"\\nProcessing dataset entries...\")\n",
    "for item in dataset['test']:  # Using test split as it contains human evaluations\n",
    "    try:\n",
    "        data.append({\n",
    "            'source': item['src'],\n",
    "            'reference': item['ref'],\n",
    "            'system_output': item['mt'],\n",
    "            'human_score': item['score'],\n",
    "            'language_pair': item['lp'],\n",
    "            'score_type': item['score_type']\n",
    "        })\n",
    "\n",
    "        if len(data) >= max_samples:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(\"\\nExample entry:\")\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Automatic Metrics\n",
    "\n",
    "Let's compute BLEU and chrF scores for each translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metrics\n",
    "print(\"\\nLoading evaluation metrics...\")\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "def calculate_metrics(row):\n",
    "    # BLEU score\n",
    "    bleu_score = bleu.compute(predictions=[row['system_output']], \n",
    "                         references=[[row['reference']]])\n",
    "    \n",
    "    # chrF score\n",
    "    chrf_score = chrf.compute(predictions=[row['system_output']], \n",
    "                         references=[[row['reference']]])\n",
    "    \n",
    "    return pd.Series({\n",
    "        'bleu': bleu_score['score'],\n",
    "        'chrf': chrf_score['score']\n",
    "    })\n",
    "\n",
    "# Calculate metrics for each row\n",
    "print(\"\\nCalculating metrics...\")\n",
    "metrics = df.apply(calculate_metrics, axis=1)\n",
    "df = pd.concat([df, metrics], axis=1)\n",
    "\n",
    "print(\"\\nMetrics summary:\")\n",
    "print(df[['bleu', 'chrf', 'human_score']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Translation Examples\n",
    "\n",
    "Let's look at some example translations and their scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSample translations with metrics:\")\n",
    "for idx, row in df.head(3).iterrows():\n",
    "    print(f\"\\nExample {idx+1}:\")\n",
    "    print(f\"Language pair: {row['language_pair']}\")\n",
    "    print(f\"Source: {row['source']}\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"System Output: {row['system_output']}\")\n",
    "    print(\"\\nScores:\")\n",
    "    print(f\"Human Score ({row['score_type']}): {row['human_score']:.3f}\")\n",
    "    print(f\"BLEU: {row['bleu']:.3f}\")\n",
    "    print(f\"chrF: {row['chrf']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Metric Correlations\n",
    "\n",
    "Let's examine how well the automatic metrics correlate with human judgments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations\n",
    "correlations = df[['bleu', 'chrf', 'human_score']].corr()\n",
    "print(\"\\nPearson correlation matrix:\")\n",
    "print(correlations)\n",
    "\n",
    "# Calculate Spearman rank correlations\n",
    "rank_correlations = df[['bleu', 'chrf', 'human_score']].corr(method='spearman')\n",
    "print(\"\\nSpearman rank correlation matrix:\")\n",
    "print(rank_correlations)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Pearson Correlations')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(rank_correlations, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Spearman Rank Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Disagreements\n",
    "\n",
    "Let's look at cases where automatic metrics strongly disagree with human judgments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized scores (z-scores) for fair comparison\n",
    "metrics = ['bleu', 'chrf', 'human_score']\n",
    "df_norm = df[metrics].apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# Find largest disagreements\n",
    "disagreements = []\n",
    "for metric in ['bleu', 'chrf']:\n",
    "    diff = abs(df_norm[metric] - df_norm['human_score'])\n",
    "    worst_idx = diff.nlargest(1).index[0]\n",
    "    \n",
    "    print(f\"\\nLargest disagreement for {metric.upper()}:\")\n",
    "    row = df.loc[worst_idx]\n",
    "    print(f\"Language pair: {row['language_pair']}\")\n",
    "    print(f\"Source: {row['source']}\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"System Output: {row['system_output']}\")\n",
    "    print(f\"Human score ({row['score_type']}): {row['human_score']:.3f}\")\n",
    "    print(f\"BLEU score: {row['bleu']:.3f}\")\n",
    "    print(f\"chrF score: {row['chrf']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Language-Specific Analysis\n",
    "\n",
    "Let's examine how metrics perform for different language pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCorrelations by language pair:\")\n",
    "for lp in df['language_pair'].unique():\n",
    "    lp_data = df[df['language_pair'] == lp]\n",
    "    if len(lp_data) > 5:  # Only show if we have enough samples\n",
    "        print(f\"\\n{lp} ({len(lp_data)} samples):\")\n",
    "        correlations = lp_data[['bleu', 'chrf', 'human_score']].corr()['human_score'][['bleu', 'chrf']]\n",
    "        print(\"Correlations with human scores:\")\n",
    "        print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Points\n",
    "\n",
    "1. Which metric correlates better with human judgments? Why might this be?\n",
    "   - Compare the Pearson and Spearman correlations\n",
    "   - Consider the differences in how BLEU and chrF work\n",
    "\n",
    "2. What types of translations tend to have high disagreement between metrics?\n",
    "   - Look at the examples with largest disagreements\n",
    "   - Consider factors like:\n",
    "     * Literal vs. natural translations\n",
    "     * Complex vs. simple sentences\n",
    "     * Cultural adaptations\n",
    "\n",
    "3. How do metrics perform across different language pairs?\n",
    "   - Look at the per-language correlations\n",
    "   - Consider linguistic differences between languages\n",
    "\n",
    "4. What are the limitations of each metric?\n",
    "   - BLEU: Focus on exact n-gram matches\n",
    "   - chrF: Character-level matching\n",
    "   - Consider what aspects of translation quality they might miss\n",
    "\n",
    "5. How could we improve automatic evaluation?\n",
    "   - Combining multiple metrics\n",
    "   - Task-specific metrics\n",
    "   - Better alignment with human judgments\n",
    "   - Neural metrics like COMET"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
