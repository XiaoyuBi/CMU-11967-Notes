{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Activity 1: Fine-tuning Using Torchtune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this activity, we'll explore examine fine-tuning approaches using [Torchtune](https://github.com/pytorch/torchtune):\n",
    "1. Tuning using LoRA\n",
    "2. Generation from the model\n",
    "3. Evaluation of the model\n",
    "\n",
    "We will use [Weights & Biases](https://wandb.ai/site) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain a Wandb API Key from here: https://wandb.ai/site\n",
    "# Then save it in the environment to visualize the results\n",
    "\n",
    "!export WANDB_API_KEY=xxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtune in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (0.5.0)\n",
      "Requirement already satisfied: datasets in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (3.2.0)\n",
      "Requirement already satisfied: huggingface_hub[hf_transfer] in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (0.28.1)\n",
      "Requirement already satisfied: safetensors in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (0.5.2)\n",
      "Requirement already satisfied: kagglehub in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (0.3.7)\n",
      "Requirement already satisfied: sentencepiece in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (0.8.0)\n",
      "Requirement already satisfied: blobfile>=2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (3.0.0)\n",
      "Requirement already satisfied: numpy in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (4.67.1)\n",
      "Requirement already satisfied: omegaconf in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (2.3.0)\n",
      "Requirement already satisfied: psutil in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (5.9.0)\n",
      "Requirement already satisfied: Pillow>=9.4.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torchtune) (11.1.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from blobfile>=2->torchtune) (2.3.0)\n",
      "Requirement already satisfied: lxml>=4.9 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from blobfile>=2->torchtune) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from blobfile>=2->torchtune) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (19.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->torchtune) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (3.11.12)\n",
      "Requirement already satisfied: packaging in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from datasets->torchtune) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
      "Requirement already satisfied: hf-transfer>=0.1.4 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
      "Requirement already satisfied: model-signing in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from kagglehub->torchtune) (0.2.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from omegaconf->torchtune) (4.9.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from tiktoken->torchtune) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\n",
      "Requirement already satisfied: cryptography in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from model-signing->kagglehub->torchtune) (44.0.0)\n",
      "Requirement already satisfied: in-toto-attestation in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from model-signing->kagglehub->torchtune) (0.9.3)\n",
      "Requirement already satisfied: sigstore in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from model-signing->kagglehub->torchtune) (3.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pandas->datasets->torchtune) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from cryptography->model-signing->kagglehub->torchtune) (1.17.1)\n",
      "Requirement already satisfied: protobuf in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from in-toto-attestation->model-signing->kagglehub->torchtune) (5.29.3)\n",
      "Requirement already satisfied: id>=1.1.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (1.5.0)\n",
      "Requirement already satisfied: pyasn1~=0.6 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (0.6.1)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (2.10.6)\n",
      "Requirement already satisfied: pyjwt>=2.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (2.10.1)\n",
      "Requirement already satisfied: pyOpenSSL>=23.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (25.0.0)\n",
      "Requirement already satisfied: rich~=13.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (13.9.4)\n",
      "Requirement already satisfied: rfc8785~=0.1.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (0.1.4)\n",
      "Requirement already satisfied: rfc3161-client~=0.1.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (0.1.2)\n",
      "Requirement already satisfied: sigstore-protobuf-specs==0.3.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (0.3.2)\n",
      "Requirement already satisfied: sigstore-rekor-types==0.0.18 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (0.0.18)\n",
      "Requirement already satisfied: tuf~=5.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (5.1.0)\n",
      "Requirement already satisfied: platformdirs~=4.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore->model-signing->kagglehub->torchtune) (4.3.6)\n",
      "Requirement already satisfied: betterproto==2.0.0b6 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub->torchtune) (2.0.0b6)\n",
      "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub->torchtune) (0.4.7)\n",
      "Requirement already satisfied: pycparser in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from cffi>=1.12->cryptography->model-signing->kagglehub->torchtune) (2.22)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pydantic<3,>=2->sigstore->model-signing->kagglehub->torchtune) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pydantic<3,>=2->sigstore->model-signing->kagglehub->torchtune) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from rich~=13.0->sigstore->model-signing->kagglehub->torchtune) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from rich~=13.0->sigstore->model-signing->kagglehub->torchtune) (2.19.1)\n",
      "Requirement already satisfied: securesystemslib~=1.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from tuf~=5.0->sigstore->model-signing->kagglehub->torchtune) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich~=13.0->sigstore->model-signing->kagglehub->torchtune) (0.1.2)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub->torchtune) (2.2.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from email-validator>=2.0.0->pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub->torchtune) (2.7.0)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub->torchtune) (4.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub->torchtune) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub->torchtune) (4.1.0)\n",
      "Requirement already satisfied: torch in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchao in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: filelock in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: wandb in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (0.19.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/gneubig/anaconda3/envs/cmu-llms/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "#Install the necessary dependencies\n",
    "!pip install torchtune\n",
    "!pip install torch torchao\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdcution to Torchtune\n",
    "Torchtune is a PyTorch library for LLM fine-tuning that prioritizes simplicity, correctness, and accessibility. It's designed to work seamlessly with PyTorch while making LLM experimentation accessible to everyone.\n",
    "\n",
    "### Recipes\n",
    "Recipes are the primary entry points for torchtune users. These can be thought of as hackable, singularly-focused scripts for interacting with LLMs including fine-tuning, inference, evaluation, and quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "#Full lst of Recipes can be found here:\n",
    "!tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download the model \n",
    "\n",
    "For this demo we will use the Qwen2.5-1.5B-Instruct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: None\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/LICENSE\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/config.json\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/.gitattributes\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/vocab.json\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/merges.txt\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/.cache\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/model.safetensors\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/README.md\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/tokenizer.json\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/torchtune_config.yaml\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/generation_config.json\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/original_repo_id.json\n",
      "/data/tir/projects/tir4/users/gneubig/work/cmu-llms-notebooks/activities/Qwen2_5-1_5B-Instruct/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "#Download the model\n",
    "\n",
    "!tune download Qwen/Qwen2.5-1.5B-Instruct \\\n",
    "--output-dir ./Qwen2_5-1_5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finetune (using LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at finetunoing using LoRA in this demo.\n",
    "\n",
    "First, getting the configs using tune cp is demonstrated.\n",
    "\n",
    "There are 2 ways to customize recipe configs:\n",
    "1. Using the `tune cp` command to copy a config from the torchtune library , modify it, and then use it when running the recipe.\n",
    "2. Specifying the changed config values in a key=value format on the command line when running the recipe. (We will use this method for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to qwen2_1_5B_lora_single_device.yaml\n"
     ]
    }
   ],
   "source": [
    "#Copying default configs for reference\n",
    "!tune cp qwen2/1.5B_lora ./qwen2_1_5B_lora_single_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directories for log outputs\n",
    "!mkdir -p qwen2_1_5B_lora_single_device_outputs\n",
    "!mkdir -p qwen2_1_5B_lora_single_device_outputs/wandb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./Qwen2_5-1_5B-Instruct\n",
      "  checkpoint_files:\n",
      "  - model.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./qwen2_1_5B_lora_single_device_outputs\n",
      "  recipe_checkpoint: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_cleaned_dataset\n",
      "  packed: false\n",
      "  split: train[:10%]\n",
      "  train_on_input: false\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 1\n",
      "gradient_accumulation_steps: 4\n",
      "log_every_n_steps: 1\n",
      "log_peak_memory_stats: true\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 5\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  group: qwen_2_5_lora_batch_4\n",
      "  job_type: lora_single_device\n",
      "  log_dir: ./qwen2_1_5B_lora_single_device_outputs/wandb_logs\n",
      "  project: tune_demo\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.lora_qwen2_1_5b\n",
      "  apply_lora_to_mlp: true\n",
      "  lora_alpha: 64\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 32\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 2.0e-05\n",
      "output_dir: ./qwen2_1_5B_lora_single_device_outputs\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: ./qwen2_1_5B_lora_single_device_outputs/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 5\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "\n",
      "Setting manual seed to local seed 2175996711. Local seed is seed + rank = 2175996711 + 0\n",
      "Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mleandermaben\u001b[0m (\u001b[33mleandermaben-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./qwen2_1_5B_lora_single_device_outputs/wandb_logs/wandb/run-20250211_051236-vr9y7rew\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-dawn-6\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/leandermaben-carnegie-mellon-university/tune_demo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/leandermaben-carnegie-mellon-university/tune_demo/runs/vr9y7rew\u001b[0m\n",
      "Logging Qwen2_5-1_5B-Instruct/torchtune_config.yaml to W&B under Files\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory allocation: 3.94 GiB\n",
      "\tGPU peak memory reserved: 4.13 GiB\n",
      "\tGPU peak memory active: 3.94 GiB\n",
      "Tokenizer is initialized from file.\n",
      "Optimizer and loss are initialized.\n",
      "Loss is initialized.\n",
      "Dataset and Sampler are initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "1|161|Loss: 1.0207405090332031: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 161/161 [06:48<00:00,  2.70s/it]Starting checkpoint save...\n",
      "Model checkpoint of size 2.88 GiB saved to qwen2_1_5B_lora_single_device_outputs/epoch_0/ft-model-00001-of-00001.safetensors\n",
      "Adapter checkpoint of size 0.07 GiB saved to qwen2_1_5B_lora_single_device_outputs/epoch_0/adapter_model.pt\n",
      "Adapter checkpoint of size 0.07 GiB saved to qwen2_1_5B_lora_single_device_outputs/epoch_0/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to qwen2_1_5B_lora_single_device_outputs/epoch_0/adapter_config.json\n",
      "Saving final epoch checkpoint.\n",
      "The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.\n",
      "Checkpoint saved in 9.06 seconds.\n",
      "1|161|Loss: 1.0207405090332031: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 161/161 [06:59<00:00,  2.60s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        peak_memory_active ‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÇ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         peak_memory_alloc ‚ñÇ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      peak_memory_reserved ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu ‚ñá‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñà‚ñÑ‚ñá‚ñÉ‚ñá‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÇ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss 1.02074\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        peak_memory_active 8.39504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         peak_memory_alloc 8.39504\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      peak_memory_reserved 23.27734\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu 1767.5188\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mfancy-dawn-6\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/leandermaben-carnegie-mellon-university/tune_demo/runs/vr9y7rew\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/leandermaben-carnegie-mellon-university/tune_demo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./qwen2_1_5B_lora_single_device_outputs/wandb_logs/wandb/run-20250211_051236-vr9y7rew/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run the recipe\n",
    "# Here we use the default configs and specify changes in the command line. We can also make changes locally and specify path to the modified config.\n",
    "# We train on 10% of the alpaca training data with a batch size of 4.\n",
    "# We log to wandb.\n",
    "\n",
    "!tune run lora_finetune_single_device \\\n",
    "    --config qwen2/1.5B_lora output_dir=\"./qwen2_1_5B_lora_single_device_outputs\" \\\n",
    "        checkpointer.checkpoint_dir=\"./Qwen2_5-1_5B-Instruct\" \\\n",
    "        tokenizer.path=\"./Qwen2_5-1_5B-Instruct/vocab.json\" \\\n",
    "        tokenizer.merges_file=\"./Qwen2_5-1_5B-Instruct/merges.txt\" \\\n",
    "        dataset.train_on_input=False \\\n",
    "        dataset.split=train[:10%] \\\n",
    "        lr_scheduler.num_warmup_steps=5 \\\n",
    "        batch_size=8 \\\n",
    "        gradient_accumulation_steps=4 \\\n",
    "        metric_logger._component_=torchtune.training.metric_logging.WandBLogger \\\n",
    "        metric_logger.project=tune_demo  metric_logger.group=qwen_2_5_lora_batch_4 \\\n",
    "        metric_logger.job_type=lora_single_device \\\n",
    "        metric_logger.log_dir=\"./qwen2_1_5B_lora_single_device_outputs/wandb_logs\" \\\n",
    "        log_every_n_steps=1 \\\n",
    "        log_peak_memory_stats=True \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "We will run inference using the `generate` recipe.\n",
    "\n",
    "First, getting the configs using tune cp is demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to generation.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune cp generation ./generation.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running InferenceRecipe with resolved config:\n",
      "\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./qwen2_1_5B_lora_single_device_outputs/epoch_0\n",
      "  checkpoint_files:\n",
      "  - ft-model-00001-of-00001.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.qwen2_1_5b\n",
      "output_dir: ./\n",
      "prompt:\n",
      "  system: null\n",
      "  user: Tell me a recipe to cook a pizza.\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "temperature: 0.6\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "  prompt_template: null\n",
      "top_k: 300\n",
      "\n",
      "Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "<|im_start|>user\n",
      "Tell me a recipe to cook a pizza.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|endoftext|>Human: How to make a pizza in 30 minutes:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "- 1/2 cup of water\n",
      "- 1/2 cup of flour\n",
      "- 1 tablespoon of olive oil\n",
      "- 1 teaspoon of salt\n",
      "- 1 teaspoon of sugar\n",
      "- 1/4 teaspoon of black pepper\n",
      "- 1/2 teaspoon of dried oregano\n",
      "- 1/4 teaspoon of dried basil\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a medium bowl, mix together the water, flour, olive oil, salt, sugar, black pepper, oregano, and basil.\n",
      "\n",
      "2. Knead the dough for about 5 minutes, until it becomes smooth and elastic.\n",
      "\n",
      "3. Place the dough into a bowl and cover with a damp cloth or plastic wrap. Let it rest for at least 30 minutes.\n",
      "\n",
      "4. Preheat your oven to 450¬∞F (232¬∞C).\n",
      "\n",
      "5. Roll out the dough into a circle, about 1/4 inch thick.\n",
      "\n",
      "6. Place the dough onto a baking sheet or pizza stone.\n",
      "\n",
      "7. Spread a tomato sauce over the dough, leaving a small border around the edges.\n",
      "\n",
      "8. Top the sauce with shredded mozzarella cheese.\n",
      "\n",
      "9. Bake in the preheated oven for 10-12 minutes, or until the cheese is melted and bubbly.\n",
      "\n",
      "10. Remove the pizza from the oven and let it cool for a few minutes.\n",
      "\n",
      "11. Slice and serve hot\n",
      "Time for inference: 7.98 sec total, 37.60 tokens/sec\n",
      "Bandwidth achieved: 151.76 GB/s\n",
      "Memory used: 4.43 GB\n"
     ]
    }
   ],
   "source": [
    "#Running inference with the finetuned model\n",
    "\n",
    "!tune run generate \\\n",
    "    --config generation \\\n",
    "    model._component_=torchtune.models.qwen2.qwen2_1_5b \\\n",
    "    tokenizer._component_=torchtune.models.qwen2.qwen2_tokenizer \\\n",
    "    tokenizer.path=\"./Qwen2_5-1_5B-Instruct/vocab.json\" \\\n",
    "    tokenizer.merges_file=./Qwen2_5-1_5B-Instruct/merges.txt \\\n",
    "    checkpointer.checkpoint_dir=./qwen2_1_5B_lora_single_device_outputs/epoch_0 \\\n",
    "    checkpointer.checkpoint_files='[ft-model-00001-of-00001.safetensors]' \\\n",
    "    checkpointer.model_type=QWEN2 \\\n",
    "    prompt.user=\"Tell me a recipe to cook a pizza.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running InferenceRecipe with resolved config:\n",
      "\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./Qwen2_5-1_5B-Instruct\n",
      "  checkpoint_files:\n",
      "  - model.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "max_new_tokens: 300\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.qwen2_1_5b\n",
      "output_dir: ./\n",
      "prompt:\n",
      "  system: null\n",
      "  user: Tell me a recipe to cook a pizza.\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "temperature: 0.6\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "  prompt_template: null\n",
      "top_k: 300\n",
      "\n",
      "Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "<|im_start|>user\n",
      "Tell me a recipe to cook a pizza.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<|endoftext|>Human: Can you please share a recipe to make pizza dough?<|im_end|>\n",
      "Time for inference: 0.65 sec total, 21.50 tokens/sec\n",
      "Bandwidth achieved: 86.78 GB/s\n",
      "Memory used: 4.16 GB\n"
     ]
    }
   ],
   "source": [
    "#Running inference with the original model\n",
    "\n",
    "!tune run generate \\\n",
    "    --config generation \\\n",
    "        model._component_=torchtune.models.qwen2.qwen2_1_5b \\\n",
    "        tokenizer._component_=torchtune.models.qwen2.qwen2_tokenizer \\\n",
    "        tokenizer.path=\"./Qwen2_5-1_5B-Instruct/vocab.json\" \\\n",
    "        tokenizer.merges_file=\"./Qwen2_5-1_5B-Instruct/merges.txt\" \\\n",
    "        checkpointer.checkpoint_dir=\"./Qwen2_5-1_5B-Instruct\" \\\n",
    "        checkpointer.checkpoint_files='[model.safetensors]' \\\n",
    "        checkpointer.model_type=QWEN2 \\\n",
    "        prompt.user=\"Tell me a recipe to cook a pizza.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate the model\n",
    "\n",
    "Torchtune integrates with lm_eval from EleutherAI for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to evaluation.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune cp qwen2/evaluation ./evaluation.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lm_eval>=0.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./qwen2_1_5B_lora_single_device_outputs/epoch_0\n",
      "  checkpoint_files:\n",
      "  - ft-model-00001-of-00001.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: 32\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.qwen2_1_5b\n",
      "output_dir: ./\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- mmlu\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "\n",
      "2025-02-11:05:48:29,989 INFO     [_utils.py:28] Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./qwen2_1_5B_lora_single_device_outputs/epoch_0\n",
      "  checkpoint_files:\n",
      "  - ft-model-00001-of-00001.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: 32\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.qwen2_1_5b\n",
      "output_dir: ./\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- mmlu\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "2025-02-11:05:48:30,810 INFO     [eleuther_eval.py:503] Model is initialized with precision torch.bfloat16.\n",
      "2025-02-11:05:48:30,986 INFO     [huggingface.py:132] Using device 'cuda:0'\n",
      "2025-02-11:05:48:31,238 INFO     [huggingface.py:369] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "Running evaluation on the following tasks: ['mmlu']\n",
      "2025-02-11:05:48:51,447 INFO     [eleuther_eval.py:540] Running evaluation on the following tasks: ['mmlu']\n",
      "2025-02-11:05:48:51,451 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 952.30it/s]\n",
      "2025-02-11:05:48:51,486 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 978.14it/s]\n",
      "2025-02-11:05:48:51,520 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.04it/s]\n",
      "2025-02-11:05:48:51,554 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 995.71it/s]\n",
      "2025-02-11:05:48:51,588 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 991.51it/s]\n",
      "2025-02-11:05:48:51,621 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 972.91it/s]\n",
      "2025-02-11:05:48:51,656 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.11it/s]\n",
      "2025-02-11:05:48:51,690 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 963.83it/s]\n",
      "2025-02-11:05:48:51,724 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 992.26it/s]\n",
      "2025-02-11:05:48:51,758 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 940.51it/s]\n",
      "2025-02-11:05:48:51,794 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 986.51it/s]\n",
      "2025-02-11:05:48:51,828 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 969.40it/s]\n",
      "2025-02-11:05:48:51,862 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 985.19it/s]\n",
      "2025-02-11:05:48:51,896 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 997.05it/s]\n",
      "2025-02-11:05:48:51,929 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 961.21it/s]\n",
      "2025-02-11:05:48:51,964 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 996.85it/s]\n",
      "2025-02-11:05:48:51,998 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 973.69it/s]\n",
      "2025-02-11:05:48:52,032 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 999.19it/s]\n",
      "2025-02-11:05:48:52,066 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1001.96it/s]\n",
      "2025-02-11:05:48:52,099 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 974.42it/s]\n",
      "2025-02-11:05:48:52,133 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 975.10it/s]\n",
      "2025-02-11:05:48:52,168 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.33it/s]\n",
      "2025-02-11:05:48:52,202 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 963.36it/s]\n",
      "2025-02-11:05:48:52,236 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 995.08it/s]\n",
      "2025-02-11:05:48:52,270 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 997.57it/s]\n",
      "2025-02-11:05:48:52,303 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1002.55it/s]\n",
      "2025-02-11:05:48:52,337 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 989.97it/s]\n",
      "2025-02-11:05:48:52,370 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 999.13it/s]\n",
      "2025-02-11:05:48:52,404 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.74it/s]\n",
      "2025-02-11:05:48:52,438 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 994.96it/s]\n",
      "2025-02-11:05:48:52,472 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 995.25it/s]\n",
      "2025-02-11:05:48:52,505 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1010.83it/s]\n",
      "2025-02-11:05:48:52,538 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 988.19it/s]\n",
      "2025-02-11:05:48:52,572 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 991.70it/s]\n",
      "2025-02-11:05:48:52,606 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1002.10it/s]\n",
      "2025-02-11:05:48:52,639 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 993.70it/s]\n",
      "2025-02-11:05:48:52,673 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 992.14it/s]\n",
      "2025-02-11:05:48:52,707 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1000.53it/s]\n",
      "2025-02-11:05:48:52,740 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 999.41it/s]\n",
      "2025-02-11:05:48:52,773 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1008.12it/s]\n",
      "2025-02-11:05:48:52,807 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 998.89it/s]\n",
      "2025-02-11:05:48:52,840 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.61it/s]\n",
      "2025-02-11:05:48:52,874 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 987.26it/s]\n",
      "2025-02-11:05:48:52,908 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.75it/s]\n",
      "2025-02-11:05:48:52,942 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 978.18it/s]\n",
      "2025-02-11:05:48:52,976 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 961.14it/s]\n",
      "2025-02-11:05:48:53,011 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 972.81it/s]\n",
      "2025-02-11:05:48:53,046 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 987.67it/s]\n",
      "2025-02-11:05:48:53,079 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1000.83it/s]\n",
      "2025-02-11:05:48:53,113 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 977.08it/s]\n",
      "2025-02-11:05:48:53,147 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 1013.39it/s]\n",
      "2025-02-11:05:48:53,180 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 987.48it/s]\n",
      "2025-02-11:05:48:53,214 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 980.15it/s]\n",
      "2025-02-11:05:48:53,249 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 996.82it/s]\n",
      "2025-02-11:05:48:53,282 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 988.38it/s]\n",
      "2025-02-11:05:48:53,316 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 993.74it/s]\n",
      "2025-02-11:05:48:53,350 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 994.73it/s]\n",
      "2025-02-11:05:48:53,383 INFO     [evaluator.py:496] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7296/7296 [00:14<00:00, 492.92it/s]\n",
      "Eval completed in 19.53 seconds.\n",
      "2025-02-11:05:49:10,981 INFO     [eleuther_eval.py:549] Eval completed in 19.53 seconds.\n",
      "Max memory allocated: 29.52 GB\n",
      "2025-02-11:05:49:10,981 INFO     [eleuther_eval.py:550] Max memory allocated: 29.52 GB\n",
      "\n",
      "\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |‚Üë  |0.6206|¬±  |0.0108|\n",
      "| - humanities                          |      2|none  |      |acc   |‚Üë  |0.6490|¬±  |0.0222|\n",
      "|  - formal_logic                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_european_history       |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - high_school_us_history             |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - high_school_world_history          |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "|  - international_law                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - jurisprudence                      |      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - logical_fallacies                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - moral_disputes                     |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - moral_scenarios                    |      1|none  |None  |acc   |‚Üë  |0.2812|¬±  |0.0808|\n",
      "|  - philosophy                         |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - prehistory                         |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - professional_law                   |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - world_religions                    |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "| - other                               |      2|none  |      |acc   |‚Üë  |0.6226|¬±  |0.0224|\n",
      "|  - business_ethics                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - clinical_knowledge                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - college_medicine                   |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - global_facts                       |      1|none  |None  |acc   |‚Üë  |0.1562|¬±  |0.0652|\n",
      "|  - human_aging                        |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - management                         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - marketing                          |      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - medical_genetics                   |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - miscellaneous                      |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - nutrition                          |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - professional_accounting            |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - professional_medicine              |      1|none  |None  |acc   |‚Üë  |0.5625|¬±  |0.0891|\n",
      "|  - virology                           |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "| - social sciences                     |      2|none  |      |acc   |‚Üë  |0.7135|¬±  |0.0229|\n",
      "|  - econometrics                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_geography              |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_government_and_politics|      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - high_school_macroeconomics         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_microeconomics         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_psychology             |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - human_sexuality                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - professional_psychology            |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - public_relations                   |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - security_studies                   |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - sociology                          |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - us_foreign_policy                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "| - stem                                |      2|none  |      |acc   |‚Üë  |0.5411|¬±  |0.0196|\n",
      "|  - abstract_algebra                   |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - anatomy                            |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - astronomy                          |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - college_biology                    |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - college_chemistry                  |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - college_computer_science           |      1|none  |None  |acc   |‚Üë  |0.5625|¬±  |0.0891|\n",
      "|  - college_mathematics                |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - college_physics                    |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - computer_security                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - conceptual_physics                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - electrical_engineering             |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - elementary_mathematics             |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - high_school_biology                |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - high_school_chemistry              |      1|none  |None  |acc   |‚Üë  |0.5625|¬±  |0.0891|\n",
      "|  - high_school_computer_science       |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - high_school_mathematics            |      1|none  |None  |acc   |‚Üë  |0.2812|¬±  |0.0808|\n",
      "|  - high_school_physics                |      1|none  |None  |acc   |‚Üë  |0.3750|¬±  |0.0870|\n",
      "|  - high_school_statistics             |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - machine_learning                   |      1|none  |None  |acc   |‚Üë  |0.3438|¬±  |0.0853|\n",
      "\n",
      "\n",
      "2025-02-11:05:49:11,643 INFO     [eleuther_eval.py:554] \n",
      "\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |‚Üë  |0.6206|¬±  |0.0108|\n",
      "| - humanities                          |      2|none  |      |acc   |‚Üë  |0.6490|¬±  |0.0222|\n",
      "|  - formal_logic                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_european_history       |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - high_school_us_history             |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - high_school_world_history          |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "|  - international_law                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - jurisprudence                      |      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - logical_fallacies                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - moral_disputes                     |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - moral_scenarios                    |      1|none  |None  |acc   |‚Üë  |0.2812|¬±  |0.0808|\n",
      "|  - philosophy                         |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - prehistory                         |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - professional_law                   |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - world_religions                    |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "| - other                               |      2|none  |      |acc   |‚Üë  |0.6226|¬±  |0.0224|\n",
      "|  - business_ethics                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - clinical_knowledge                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - college_medicine                   |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - global_facts                       |      1|none  |None  |acc   |‚Üë  |0.1562|¬±  |0.0652|\n",
      "|  - human_aging                        |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - management                         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - marketing                          |      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - medical_genetics                   |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - miscellaneous                      |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - nutrition                          |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - professional_accounting            |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - professional_medicine              |      1|none  |None  |acc   |‚Üë  |0.5625|¬±  |0.0891|\n",
      "|  - virology                           |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "| - social sciences                     |      2|none  |      |acc   |‚Üë  |0.7135|¬±  |0.0229|\n",
      "|  - econometrics                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_geography              |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_government_and_politics|      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - high_school_macroeconomics         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_microeconomics         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_psychology             |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - human_sexuality                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - professional_psychology            |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - public_relations                   |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - security_studies                   |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - sociology                          |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - us_foreign_policy                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "| - stem                                |      2|none  |      |acc   |‚Üë  |0.5411|¬±  |0.0196|\n",
      "|  - abstract_algebra                   |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - anatomy                            |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - astronomy                          |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - college_biology                    |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - college_chemistry                  |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - college_computer_science           |      1|none  |None  |acc   |‚Üë  |0.5625|¬±  |0.0891|\n",
      "|  - college_mathematics                |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - college_physics                    |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - computer_security                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - conceptual_physics                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - electrical_engineering             |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - elementary_mathematics             |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - high_school_biology                |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - high_school_chemistry              |      1|none  |None  |acc   |‚Üë  |0.5625|¬±  |0.0891|\n",
      "|  - high_school_computer_science       |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - high_school_mathematics            |      1|none  |None  |acc   |‚Üë  |0.2812|¬±  |0.0808|\n",
      "|  - high_school_physics                |      1|none  |None  |acc   |‚Üë  |0.3750|¬±  |0.0870|\n",
      "|  - high_school_statistics             |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - machine_learning                   |      1|none  |None  |acc   |‚Üë  |0.3438|¬±  |0.0853|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the finetuned model on the mmlu task\n",
    "\n",
    "!tune run eleuther_eval --config qwen2/evaluation \\\n",
    "    model._component_=torchtune.models.qwen2.qwen2_1_5b \\\n",
    "    tokenizer.path=\"./Qwen2_5-1_5B-Instruct/vocab.json\" \\\n",
    "    tokenizer.merges_file=\"./Qwen2_5-1_5B-Instruct/merges.txt\" \\\n",
    "    checkpointer.checkpoint_dir=\"./qwen2_1_5B_lora_single_device_outputs/epoch_0\" \\\n",
    "    checkpointer.checkpoint_files='[ft-model-00001-of-00001.safetensors]' \\\n",
    "    tasks=[\"mmlu\"] \\\n",
    "    limit=32 \\\n",
    "    batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./Qwen2_5-1_5B-Instruct\n",
      "  checkpoint_files:\n",
      "  - model.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: 32\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.qwen2_1_5b\n",
      "output_dir: ./\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- mmlu\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "\n",
      "2025-02-11:05:49:53,105 INFO     [_utils.py:28] Running EleutherEvalRecipe with resolved config:\n",
      "\n",
      "batch_size: 4\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: ./Qwen2_5-1_5B-Instruct\n",
      "  checkpoint_files:\n",
      "  - model.safetensors\n",
      "  model_type: QWEN2\n",
      "  output_dir: ./\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_kv_cache: true\n",
      "limit: 32\n",
      "max_seq_length: 4096\n",
      "model:\n",
      "  _component_: torchtune.models.qwen2.qwen2_1_5b\n",
      "output_dir: ./\n",
      "quantizer: null\n",
      "seed: 1234\n",
      "tasks:\n",
      "- mmlu\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.qwen2.qwen2_tokenizer\n",
      "  max_seq_len: null\n",
      "  merges_file: ./Qwen2_5-1_5B-Instruct/merges.txt\n",
      "  path: ./Qwen2_5-1_5B-Instruct/vocab.json\n",
      "\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "2025-02-11:05:49:54,068 INFO     [eleuther_eval.py:503] Model is initialized with precision torch.bfloat16.\n",
      "2025-02-11:05:49:54,247 INFO     [huggingface.py:132] Using device 'cuda:0'\n",
      "2025-02-11:05:49:54,580 INFO     [huggingface.py:369] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "Running evaluation on the following tasks: ['mmlu']\n",
      "2025-02-11:05:50:30,555 INFO     [eleuther_eval.py:540] Running evaluation on the following tasks: ['mmlu']\n",
      "2025-02-11:05:50:30,558 INFO     [task.py:415] Building contexts for mmlu_college_biology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 951.03it/s]\n",
      "2025-02-11:05:50:30,593 INFO     [task.py:415] Building contexts for mmlu_abstract_algebra on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 978.90it/s]\n",
      "2025-02-11:05:50:30,627 INFO     [task.py:415] Building contexts for mmlu_machine_learning on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 973.96it/s]\n",
      "2025-02-11:05:50:30,661 INFO     [task.py:415] Building contexts for mmlu_college_physics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 980.01it/s]\n",
      "2025-02-11:05:50:30,696 INFO     [task.py:415] Building contexts for mmlu_high_school_mathematics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 975.02it/s]\n",
      "2025-02-11:05:50:30,730 INFO     [task.py:415] Building contexts for mmlu_college_computer_science on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 959.75it/s]\n",
      "2025-02-11:05:50:30,765 INFO     [task.py:415] Building contexts for mmlu_astronomy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 965.51it/s]\n",
      "2025-02-11:05:50:30,799 INFO     [task.py:415] Building contexts for mmlu_conceptual_physics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 960.18it/s]\n",
      "2025-02-11:05:50:30,834 INFO     [task.py:415] Building contexts for mmlu_electrical_engineering on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 951.70it/s]\n",
      "2025-02-11:05:50:30,869 INFO     [task.py:415] Building contexts for mmlu_anatomy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 974.01it/s]\n",
      "2025-02-11:05:50:30,904 INFO     [task.py:415] Building contexts for mmlu_elementary_mathematics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 981.66it/s]\n",
      "2025-02-11:05:50:30,938 INFO     [task.py:415] Building contexts for mmlu_college_mathematics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 966.26it/s]\n",
      "2025-02-11:05:50:30,972 INFO     [task.py:415] Building contexts for mmlu_high_school_statistics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 982.58it/s]\n",
      "2025-02-11:05:50:31,006 INFO     [task.py:415] Building contexts for mmlu_college_chemistry on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 966.53it/s]\n",
      "2025-02-11:05:50:31,041 INFO     [task.py:415] Building contexts for mmlu_high_school_physics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 944.62it/s]\n",
      "2025-02-11:05:50:31,076 INFO     [task.py:415] Building contexts for mmlu_high_school_chemistry on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 960.94it/s]\n",
      "2025-02-11:05:50:31,111 INFO     [task.py:415] Building contexts for mmlu_computer_security on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 971.09it/s]\n",
      "2025-02-11:05:50:31,145 INFO     [task.py:415] Building contexts for mmlu_high_school_biology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 963.40it/s]\n",
      "2025-02-11:05:50:31,180 INFO     [task.py:415] Building contexts for mmlu_high_school_computer_science on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 956.70it/s]\n",
      "2025-02-11:05:50:31,215 INFO     [task.py:415] Building contexts for mmlu_professional_medicine on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 974.39it/s]\n",
      "2025-02-11:05:50:31,249 INFO     [task.py:415] Building contexts for mmlu_college_medicine on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 961.30it/s]\n",
      "2025-02-11:05:50:31,284 INFO     [task.py:415] Building contexts for mmlu_virology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 974.37it/s]\n",
      "2025-02-11:05:50:31,318 INFO     [task.py:415] Building contexts for mmlu_miscellaneous on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 954.22it/s]\n",
      "2025-02-11:05:50:31,353 INFO     [task.py:415] Building contexts for mmlu_management on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 964.64it/s]\n",
      "2025-02-11:05:50:31,388 INFO     [task.py:415] Building contexts for mmlu_professional_accounting on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 957.04it/s]\n",
      "2025-02-11:05:50:31,423 INFO     [task.py:415] Building contexts for mmlu_human_aging on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.41it/s]\n",
      "2025-02-11:05:50:31,456 INFO     [task.py:415] Building contexts for mmlu_global_facts on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 983.16it/s]\n",
      "2025-02-11:05:50:31,490 INFO     [task.py:415] Building contexts for mmlu_business_ethics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 985.77it/s]\n",
      "2025-02-11:05:50:31,524 INFO     [task.py:415] Building contexts for mmlu_marketing on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 965.02it/s]\n",
      "2025-02-11:05:50:31,559 INFO     [task.py:415] Building contexts for mmlu_nutrition on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 977.24it/s]\n",
      "2025-02-11:05:50:31,593 INFO     [task.py:415] Building contexts for mmlu_clinical_knowledge on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 980.97it/s]\n",
      "2025-02-11:05:50:31,627 INFO     [task.py:415] Building contexts for mmlu_medical_genetics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 986.36it/s]\n",
      "2025-02-11:05:50:31,661 INFO     [task.py:415] Building contexts for mmlu_high_school_government_and_politics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 974.11it/s]\n",
      "2025-02-11:05:50:31,695 INFO     [task.py:415] Building contexts for mmlu_high_school_psychology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 960.27it/s]\n",
      "2025-02-11:05:50:31,730 INFO     [task.py:415] Building contexts for mmlu_high_school_geography on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 970.71it/s]\n",
      "2025-02-11:05:50:31,764 INFO     [task.py:415] Building contexts for mmlu_security_studies on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 959.64it/s]\n",
      "2025-02-11:05:50:31,799 INFO     [task.py:415] Building contexts for mmlu_sociology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 956.89it/s]\n",
      "2025-02-11:05:50:31,834 INFO     [task.py:415] Building contexts for mmlu_professional_psychology on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 947.16it/s]\n",
      "2025-02-11:05:50:31,869 INFO     [task.py:415] Building contexts for mmlu_high_school_microeconomics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 966.46it/s]\n",
      "2025-02-11:05:50:31,904 INFO     [task.py:415] Building contexts for mmlu_econometrics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 967.70it/s]\n",
      "2025-02-11:05:50:31,938 INFO     [task.py:415] Building contexts for mmlu_high_school_macroeconomics on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 954.38it/s]\n",
      "2025-02-11:05:50:31,973 INFO     [task.py:415] Building contexts for mmlu_public_relations on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 977.06it/s]\n",
      "2025-02-11:05:50:32,007 INFO     [task.py:415] Building contexts for mmlu_us_foreign_policy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 974.19it/s]\n",
      "2025-02-11:05:50:32,042 INFO     [task.py:415] Building contexts for mmlu_human_sexuality on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 972.42it/s]\n",
      "2025-02-11:05:50:32,076 INFO     [task.py:415] Building contexts for mmlu_prehistory on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 960.78it/s]\n",
      "2025-02-11:05:50:32,111 INFO     [task.py:415] Building contexts for mmlu_logical_fallacies on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 981.00it/s]\n",
      "2025-02-11:05:50:32,145 INFO     [task.py:415] Building contexts for mmlu_high_school_world_history on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 959.53it/s]\n",
      "2025-02-11:05:50:32,179 INFO     [task.py:415] Building contexts for mmlu_professional_law on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 967.85it/s]\n",
      "2025-02-11:05:50:32,214 INFO     [task.py:415] Building contexts for mmlu_world_religions on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 975.92it/s]\n",
      "2025-02-11:05:50:32,248 INFO     [task.py:415] Building contexts for mmlu_moral_disputes on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 952.43it/s]\n",
      "2025-02-11:05:50:32,283 INFO     [task.py:415] Building contexts for mmlu_moral_scenarios on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 979.98it/s]\n",
      "2025-02-11:05:50:32,317 INFO     [task.py:415] Building contexts for mmlu_international_law on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 963.11it/s]\n",
      "2025-02-11:05:50:32,352 INFO     [task.py:415] Building contexts for mmlu_formal_logic on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 956.08it/s]\n",
      "2025-02-11:05:50:32,387 INFO     [task.py:415] Building contexts for mmlu_philosophy on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 949.15it/s]\n",
      "2025-02-11:05:50:32,422 INFO     [task.py:415] Building contexts for mmlu_high_school_european_history on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 980.00it/s]\n",
      "2025-02-11:05:50:32,457 INFO     [task.py:415] Building contexts for mmlu_jurisprudence on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 954.61it/s]\n",
      "2025-02-11:05:50:32,491 INFO     [task.py:415] Building contexts for mmlu_high_school_us_history on rank 0...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:00<00:00, 955.93it/s]\n",
      "2025-02-11:05:50:32,526 INFO     [evaluator.py:496] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7296/7296 [00:14<00:00, 487.76it/s]\n",
      "Eval completed in 19.80 seconds.\n",
      "2025-02-11:05:50:50,355 INFO     [eleuther_eval.py:549] Eval completed in 19.80 seconds.\n",
      "Max memory allocated: 29.52 GB\n",
      "2025-02-11:05:50:50,356 INFO     [eleuther_eval.py:550] Max memory allocated: 29.52 GB\n",
      "\n",
      "\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |‚Üë  |0.6157|¬±  |0.0108|\n",
      "| - humanities                          |      2|none  |      |acc   |‚Üë  |0.6442|¬±  |0.0222|\n",
      "|  - formal_logic                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_european_history       |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_us_history             |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - high_school_world_history          |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "|  - international_law                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - jurisprudence                      |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - logical_fallacies                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - moral_disputes                     |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - moral_scenarios                    |      1|none  |None  |acc   |‚Üë  |0.2500|¬±  |0.0778|\n",
      "|  - philosophy                         |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - prehistory                         |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - professional_law                   |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - world_religions                    |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "| - other                               |      2|none  |      |acc   |‚Üë  |0.6154|¬±  |0.0223|\n",
      "|  - business_ethics                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - clinical_knowledge                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - college_medicine                   |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - global_facts                       |      1|none  |None  |acc   |‚Üë  |0.1250|¬±  |0.0594|\n",
      "|  - human_aging                        |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - management                         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - marketing                          |      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - medical_genetics                   |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - miscellaneous                      |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - nutrition                          |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - professional_accounting            |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - professional_medicine              |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - virology                           |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "| - social sciences                     |      2|none  |      |acc   |‚Üë  |0.7135|¬±  |0.0228|\n",
      "|  - econometrics                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_geography              |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_government_and_politics|      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "|  - high_school_macroeconomics         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_microeconomics         |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - high_school_psychology             |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - human_sexuality                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - professional_psychology            |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - public_relations                   |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - security_studies                   |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - sociology                          |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - us_foreign_policy                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "| - stem                                |      2|none  |      |acc   |‚Üë  |0.5345|¬±  |0.0196|\n",
      "|  - abstract_algebra                   |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - anatomy                            |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - astronomy                          |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - college_biology                    |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - college_chemistry                  |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - college_computer_science           |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - college_mathematics                |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - college_physics                    |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - computer_security                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - conceptual_physics                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - electrical_engineering             |      1|none  |None  |acc   |‚Üë  |0.6250|¬±  |0.0870|\n",
      "|  - elementary_mathematics             |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - high_school_biology                |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - high_school_chemistry              |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - high_school_computer_science       |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - high_school_mathematics            |      1|none  |None  |acc   |‚Üë  |0.3125|¬±  |0.0832|\n",
      "|  - high_school_physics                |      1|none  |None  |acc   |‚Üë  |0.3438|¬±  |0.0853|\n",
      "|  - high_school_statistics             |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - machine_learning                   |      1|none  |None  |acc   |‚Üë  |0.3438|¬±  |0.0853|\n",
      "\n",
      "\n",
      "2025-02-11:05:50:50,510 INFO     [eleuther_eval.py:554] \n",
      "\n",
      "|                 Tasks                 |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|---------------------------------------|------:|------|------|------|---|-----:|---|-----:|\n",
      "|mmlu                                   |      2|none  |      |acc   |‚Üë  |0.6157|¬±  |0.0108|\n",
      "| - humanities                          |      2|none  |      |acc   |‚Üë  |0.6442|¬±  |0.0222|\n",
      "|  - formal_logic                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_european_history       |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_us_history             |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - high_school_world_history          |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "|  - international_law                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - jurisprudence                      |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - logical_fallacies                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - moral_disputes                     |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - moral_scenarios                    |      1|none  |None  |acc   |‚Üë  |0.2500|¬±  |0.0778|\n",
      "|  - philosophy                         |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - prehistory                         |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - professional_law                   |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - world_religions                    |      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "| - other                               |      2|none  |      |acc   |‚Üë  |0.6154|¬±  |0.0223|\n",
      "|  - business_ethics                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - clinical_knowledge                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - college_medicine                   |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - global_facts                       |      1|none  |None  |acc   |‚Üë  |0.1250|¬±  |0.0594|\n",
      "|  - human_aging                        |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - management                         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - marketing                          |      1|none  |None  |acc   |‚Üë  |0.8438|¬±  |0.0652|\n",
      "|  - medical_genetics                   |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - miscellaneous                      |      1|none  |None  |acc   |‚Üë  |0.5938|¬±  |0.0882|\n",
      "|  - nutrition                          |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - professional_accounting            |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - professional_medicine              |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - virology                           |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "| - social sciences                     |      2|none  |      |acc   |‚Üë  |0.7135|¬±  |0.0228|\n",
      "|  - econometrics                       |      1|none  |None  |acc   |‚Üë  |0.5000|¬±  |0.0898|\n",
      "|  - high_school_geography              |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_government_and_politics|      1|none  |None  |acc   |‚Üë  |0.8750|¬±  |0.0594|\n",
      "|  - high_school_macroeconomics         |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - high_school_microeconomics         |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - high_school_psychology             |      1|none  |None  |acc   |‚Üë  |0.8125|¬±  |0.0701|\n",
      "|  - human_sexuality                    |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - professional_psychology            |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - public_relations                   |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - security_studies                   |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - sociology                          |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - us_foreign_policy                  |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "| - stem                                |      2|none  |      |acc   |‚Üë  |0.5345|¬±  |0.0196|\n",
      "|  - abstract_algebra                   |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - anatomy                            |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - astronomy                          |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - college_biology                    |      1|none  |None  |acc   |‚Üë  |0.7188|¬±  |0.0808|\n",
      "|  - college_chemistry                  |      1|none  |None  |acc   |‚Üë  |0.4062|¬±  |0.0882|\n",
      "|  - college_computer_science           |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - college_mathematics                |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - college_physics                    |      1|none  |None  |acc   |‚Üë  |0.4688|¬±  |0.0896|\n",
      "|  - computer_security                  |      1|none  |None  |acc   |‚Üë  |0.7812|¬±  |0.0742|\n",
      "|  - conceptual_physics                 |      1|none  |None  |acc   |‚Üë  |0.6562|¬±  |0.0853|\n",
      "|  - electrical_engineering             |      1|none  |None  |acc   |‚Üë  |0.6250|¬±  |0.0870|\n",
      "|  - elementary_mathematics             |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - high_school_biology                |      1|none  |None  |acc   |‚Üë  |0.6875|¬±  |0.0832|\n",
      "|  - high_school_chemistry              |      1|none  |None  |acc   |‚Üë  |0.5312|¬±  |0.0896|\n",
      "|  - high_school_computer_science       |      1|none  |None  |acc   |‚Üë  |0.7500|¬±  |0.0778|\n",
      "|  - high_school_mathematics            |      1|none  |None  |acc   |‚Üë  |0.3125|¬±  |0.0832|\n",
      "|  - high_school_physics                |      1|none  |None  |acc   |‚Üë  |0.3438|¬±  |0.0853|\n",
      "|  - high_school_statistics             |      1|none  |None  |acc   |‚Üë  |0.4375|¬±  |0.0891|\n",
      "|  - machine_learning                   |      1|none  |None  |acc   |‚Üë  |0.3438|¬±  |0.0853|\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the original model on the mmlu task\n",
    "\n",
    "!tune run eleuther_eval --config qwen2/evaluation \\\n",
    "    model._component_=torchtune.models.qwen2.qwen2_1_5b \\\n",
    "    tokenizer.path=\"./Qwen2_5-1_5B-Instruct/vocab.json\" \\\n",
    "    tokenizer.merges_file=\"./Qwen2_5-1_5B-Instruct/merges.txt\" \\\n",
    "    checkpointer.checkpoint_dir=\"./Qwen2_5-1_5B-Instruct\" \\\n",
    "    checkpointer.checkpoint_files='[model.safetensors]' \\\n",
    "    tasks=[\"mmlu\"] \\\n",
    "    limit=32 \\\n",
    "    batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llma_tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
